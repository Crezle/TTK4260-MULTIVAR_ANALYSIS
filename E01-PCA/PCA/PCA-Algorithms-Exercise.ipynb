{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: To develop a mathematical understanding of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg as LA\n",
    "X = np.array([\n",
    "        [0.387,4878, 5.42],\n",
    "        [0.723,12104,5.25],\n",
    "        [1,12756,5.52],\n",
    "        [1.524,6787,3.94],\n",
    "    ])\n",
    "X = X - np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Iinear Iterative Partial Least-Squares (NIPALS) algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to compute PCA using NIPALS algorithm\n",
    "\n",
    "* Step 1: Initialize an arbitrary column vector $\\mathbf{t}_{a}$ either randomly or by just copying any column of X. \n",
    "* Step 2: Take very column of $\\mathbf{X}$, $\\mathbf{X_k}$ and regress it onto the $\\mathbf{t}_{a}$ vector and store the regression coefficeints as $\\mathbf{p}_{ka}$. (Note: This simply means performing an ordinary least squares regression ($y=mx$) with $x=t_{a}$ and $y=X_{k}$ with $m=(\\mathbf{x^T}\\mathbf{x})^{-1}\\mathbf{x^T}\\mathbf{y}$). In the current notation we get \n",
    "$$p_{ka}=\\frac{\\mathbf{t_a^T}\\mathbf{X}_{k}}{\\mathbf{t_a^T}\\mathbf{t_a}}$$\n",
    "\n",
    "Repeat it for each of the columns of $\\mathbf{X}$ to get the entire vector $\\mathbf{p}_{k}$. This is shown in the illustration\n",
    "above where each column from $X$ is regressed, one at a time, on $\\mathbf{t}_{a}$, to calculate the loading entry, $\\mathbf{ùëù}_{ka}$ \n",
    "\n",
    "In practice we don‚Äôt do this one column at time; we can regress all columns in $X$ in go: $$\\mathbf{p_a^T}=\\frac{1}{\\mathbf{t_a^T}\\mathbf{t_a}}.\\mathbf{t_a^T}\\mathbf{X_a}$$  where $\\mathbf{t_a}$ is an $N \\times 1$ column vector, and $\\mathbf{X}_{a}$ us an $N \\times K$ matrix.\n",
    "* The loading vector $\\mathbf{p_a^T}$ won‚Äôt have unit length (magnitude) yet. So we simply rescale it to have\n",
    "magnitude of 1.0: $$\\mathbf{p_a^T}=\\frac{\\mathbf{p_a^T}}{\\sqrt{\\mathbf{p_a^T}\\mathbf{p_a}}}$$\n",
    "* Step 4: Regress every row in $X$ onto this normalized loadings vector. As illustrated below, in our linear regression the rows in X are our y-variable each time, while the loadings vector is our x-variable. The regression coefficient becomes the score value for that $ùëñ^{th}$ row:\n",
    "\n",
    "$$p_{i,a}=\\frac{\\mathbf{x}_{i}^{T}\\mathbf{p}_{a}}{\\mathbf{p}_{a}^{T}\\mathbf{p}_{a}}$$\n",
    "where $x_{i}^{T}$ is a $K \\times 1$ column vector. We can combine these $N$ separate least-squares models and\n",
    "calculate them in one go to get the entire vector, \n",
    "\n",
    "$$\\mathbf{t}_{a}^{T}=\\frac{1}{\\mathbf{p}_{a}^{T}\\mathbf{p}_{a}}\\mathbf{X}\\mathbf{p}_{a}^{T}$$ \n",
    "\n",
    "where $p_{a}$ is a $K \\times 1$ column vector.\n",
    "\n",
    "* Step 5: Continue looping over steps 2,3,4 until the change in vector $t_{a}$ is below a chosen tolerance\n",
    "* Step 6: On convergence, the score vector and the loading vectors, $\\mathbf{t}_{a}$ and $\\mathbf{p}_{a}$ are stored as the $a^{th}$ column in matrix $\\mathbf{T}$ and $\\mathbf{P}$. We then deflate the $\\mathbf{X}$ matrix. This crucial step removes the variability captured in this component ($t_{a}$ and $p_{a}$) from $\\mathbf{X}$:\n",
    "\n",
    "$$E_{a}=X_{a}-t_{a}p_{a}^{T}$$\n",
    "\n",
    "$$X_{a+1} = E_{a}$$ \n",
    "\n",
    "For the first component, $X_{a}$ is just the preprocessed raw data. So we can see that the second component is actually calculated on the residuals $E_{1}$, obtained after extracting the first component. This is called deflation, and nicely shows why each component is orthogonal to the others. Each subsequent component is only seeing variation remaining after removing all the others; there is no possibility that two components can explain the same type of variability. After deflation we go back to step 1 and repeat the entire process for the next component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTATION IN PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X,no_components):\n",
    "    tol = 0.0000001\n",
    "    it=1000\n",
    "    obsCount,varCount = X.shape\n",
    "    Xa = X - np.mean(X, axis = 0) \n",
    "    #Xh = X-np.tile(np.mean(X,axis=0).reshape(-1,1).T, obsCount).reshape(4,3)\n",
    "    T = np.zeros((obsCount,no_components))\n",
    "    P = np.zeros((varCount,no_components))\n",
    "    pcvar = np.zeros((varCount,1))\n",
    "    varTotal = np.sum(np.var(Xa,axis=0,ddof=1))\n",
    "    currVar = varTotal\n",
    "    nr=0\n",
    "    for h in range(no_components):\n",
    "        th = Xa[:,0].reshape(-1,1)\n",
    "        ende = False\n",
    "        while ende != True:\n",
    "            nr = nr + 1\n",
    "            ph = np.dot(Xa.T,th)/np.dot(th.T,th)\n",
    "            ph = ph /np.linalg.norm(ph)\n",
    "            thnew = np.dot(Xa,ph)/np.dot(ph.T,ph)\n",
    "            prec = np.dot((thnew-th).T,(thnew-th))\n",
    "            th = thnew\n",
    "            if prec <= (tol*tol):\n",
    "                ende = True\n",
    "            elif it <=nr:\n",
    "                ende = True\n",
    "                print(\"Iternation stops without convergence\")\n",
    "        Ea = Xa - np.dot(th,ph.T)\n",
    "        Xa = Ea    \n",
    "        T[:,h] = th.flatten()\n",
    "        P[:,h] = ph.flatten()\n",
    "        oldVar = currVar\n",
    "        currVar = np.sum(np.var(Xa,axis=0,ddof=1))\n",
    "        pcvar[h] = (oldVar - currVar) / varTotal\n",
    "        nr = 0\n",
    "    return T,P,pcvar      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of the NIPALS algorithm\n",
    "* The NIPALS algorithm computes one component at a time. The first component computed is\n",
    "equivalent to the t1 and p1 vectors that would have been found from an eigenvalue or singular value\n",
    "decomposition.\n",
    "* The algorithm can handle missing data in X.\n",
    "* The algorithm always converges, but the convergence can sometimes be slow.\n",
    "* It is also known as the Power algorithm to calculate eigenvectors and eigenvalues.\n",
    "* It works well for very large data sets.\n",
    "* It is used by most software packages, especially those that handle missing data.\n",
    "* Of interest: it is well known that Google used this algorithm for the early versions of their search engine, called PageRank148."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_components=3\n",
    "T,P,pcvar = PCA(X,no_components)\n",
    "print(\"T (Scores)\")\n",
    "print(T)\n",
    "print(\" \")\n",
    "print(\"P (Loadings)\")\n",
    "print(P)\n",
    "print(np.sqrt(pcvar)/np.sum(np.sqrt(pcvar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd \n",
    "U, S, PTrans = svd(X, full_matrices=False)\n",
    "Sigma = np.diag(S)\n",
    "T=np.dot(U,Sigma)\n",
    "P=PTrans.T\n",
    "\n",
    "print(\"T (Scores)\")\n",
    "print(T)\n",
    "print(\" \")\n",
    "print(\"P (Loadings)\")\n",
    "print(P)\n",
    "print(\"Sigma (Variance)\")\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()  \n",
    "T=pca.fit_transform(X)\n",
    "Prans=pca.components_ #eigen vectors.T\n",
    "latent = pca.explained_variance_\n",
    "explained = pca.explained_variance_ratio_\n",
    "P=PTrans.T\n",
    "S=pca.singular_values_\n",
    "Sigma=np.diag(S)\n",
    "print(\"T (Scores)\")\n",
    "print(T)\n",
    "print(\" \")\n",
    "print(\"P (Loadings)\")\n",
    "print(P)\n",
    "print(\"Sigma (Variance)\")\n",
    "print(S)\n",
    "#print(pca.singular_values_/np.sqrt(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_2 = (S ** 2) / 4\n",
    "explained_variance_ratio_2 = (explained_variance_2 / explained_variance_2.sum())\n",
    "print(explained_variance_ratio_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue decomposition approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the latent variable directions (the loading vectors) were oriented so that the variance of the\n",
    "scores in that direction were maximal. We can cast this as an optimization problem. For the first\n",
    "component: $$max (\\phi)=\\mathbf{t_1^T}\\mathbf{t_1}=\\mathbf{p_1^T} \\mathbf{X^T}\\mathbf{Xp_1}$$\n",
    "such that $$\\mathbf{p_1^T p_1}=1$$.\n",
    "\n",
    "This is equivalent to $$max (\\phi)=\\mathbf{p_1^T} \\mathbf{X^T Xp_1}-\\lambda(\\mathbf{p_1^T}\\mathbf{p_1}-1)$$ \n",
    "\n",
    "because we can move the constraint into the objective function with a Lagrange multiplier, $\\lambda$. The maximum value must occur when the partial derivatives with respect to $\\mathbf{p_1}$, \n",
    "\n",
    "our search variable, are zero: $$\\frac{\\partial \\phi}{\\partial p_1}= \\frac{\\partial (\\mathbf{p_1^T X^T Xp_1}-\\lambda(\\mathbf{p}_{1}^{T}\\mathbf{p}_{1}-1))}{\\partial \\mathbf{p}_1}=0$$\n",
    "\n",
    "$$2\\mathbf{X^T X p_1}-2\\lambda_1\\mathbf{p_1}=0$$\n",
    "\n",
    "$$(\\mathbf{X^TX}-\\lambda_1\\mathbf{I})\\mathbf{p_1}=0$$\n",
    "\n",
    "$$\\mathbf{X^T Xp_1}=\\lambda_{1}\\mathbf{p_1}$$\n",
    "\n",
    "which is just the eigenvalue equation, indicating that $\\mathbf{p_1}$ is the eigenvector of $\\mathbf{X^T X}$ and $\\lambda_1$ is the eigenvalue. One can show that $\\lambda_1=\\mathbf{t_1^T t_1}$, which is proportional to the variance of the first component. In a similar manner we can calculate the second eigenvalue, but this time we add the additional constraint that $\\mathbf{p}_1 \\perp \\mathbf{p}_2$. Writing out this objective function and taking partial derivatives leads to showing that \n",
    "\n",
    "$$\\mathbf{X^{T}Xp_{2}} = \\lambda_{2} \\mathbf{p_2}$$\n",
    "\n",
    "From this we learn that:\n",
    "* The loadings are the eigenvectors of $\\mathbf{X^TX}$.\n",
    "* Sorting the eigenvalues in order from largest to smallest gives the order of the corresponding eigenvectors, the loadings.\n",
    "* We know from the theory of eigenvalues that if there are distinct eigenvalues, then their eigenvectors are linearly independent (orthogonal).\n",
    "* We also know the eigenvalues of $\\mathbf{X^TX}$ must be real values and positive; this matches with the interpretation that the eigenvalues are proportional to the variance of each score vector.\n",
    "* Also, the sum of the eigenvalues must add up to sum of the diagonal entries of $\\mathbf{X^TX}$, which represents of the total variance of the $\\mathbf{X}$ matrix, if all eigenvectors are extracted. So plotting the eigenvalues is equivalent to showing the proportion of variance explained in X by each component. This is not necessarily a good way to judge the number of components to use, but it is a rough guide: use a Pareto plot of the eigenvalues (though in the context of eigenvalue problems, this plot is called a scree plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(X, rowvar = False)\n",
    "evals , P = LA.eigh(cov)\n",
    "idx = np.argsort(evals)[::-1]\n",
    "P = P[:,idx]\n",
    "evals = evals[idx]\n",
    "T = np.dot(X, P) \n",
    "Sigma=LA.norm(T,axis=0)\n",
    "print(\"T (Scores)\")\n",
    "print(T)\n",
    "print(\"P (Loadings)\")\n",
    "print(P)\n",
    "print(\"Sigma (Variance)\")\n",
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Test if the loading vectors are orthogonal and orthonormal or not"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" Your code here \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" ---         ---\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Test if the scores vectors are orthogonal and orthonormal or not"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" Your code here \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" ---         ---\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Add more columns to the original data matrix by: \n",
    "* Make some of the columns to be the linear combination of others\n",
    "* Duplicate some columns\n",
    "* Add noise as some columns \n",
    "* Add a few columns of categorical values\n",
    "\n",
    "Then apply PCA to the dataset and report your findings here"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" Your code here \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" ---         ---\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Find the PCA of the original data without mean centering and report the findings"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" Your code here \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" ---         ---\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "50fd1a218e5a297a403bb8323478fe363865c88bef83b839c84125d9b5b7e1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
